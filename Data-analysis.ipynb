{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARDDoS Data analysis for TFTP, SNMP and SSDP test traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# For showing progress bar of for loops\n",
    "from progressbar import Bar, ETA, \\\n",
    "    AdaptiveETA, Percentage, \\\n",
    "    ProgressBar \n",
    "widgets = [Percentage(),\n",
    "            ' ', Bar(),\n",
    "            ' ', ETA(),\n",
    "            ' ', AdaptiveETA()]\n",
    "pbar = ProgressBar(widgets=widgets)\n",
    "\n",
    "# Own functions\n",
    "from pcapreader import PcapReader\n",
    "pcapToDf = PcapReader.pcapToDf\n",
    "\n",
    "# For converting string to ip address in dataframe\n",
    "from cyberpandas import to_ipaddress \n",
    "# For pretty printing dataframes\n",
    "from tabulate import tabulate \n",
    "# For plotting\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refineLatexTableStringScientificPaper(latexTableString):\n",
    "    # Hacks for multiindex dataframe\n",
    "    multiColFirst = latexTableString.find(\"\\multicolumn\")\n",
    "    latexTableString = latexTableString[:multiColFirst+16] + 'c' + latexTableString[multiColFirst+17:]\n",
    "    startRemove = latexTableString.find('\\multicolumn',multiColFirst+1)\n",
    "    latexTableString = latexTableString[:startRemove] + latexTableString[startRemove+18:]\n",
    "    # Convert Level and Amplification factor to multirow:\n",
    "    latexTableString = latexTableString[:latexTableString.find('Level')] + '\\\\multirow{2}{*}{Level}' +  latexTableString[latexTableString.find('Level')+len('Level'):]\n",
    "    latexTableString = latexTableString[:latexTableString.find('Amplification factor')] + '\\\\multirow{2}{*}{Amplification Factor}' +  latexTableString[latexTableString.find('Amplification factor')+len('Amplification factor'):]\n",
    "    # Adding a horizontal line for multindex table\n",
    "    latexTableString = latexTableString[:latexTableString.find('\\\\\\\\')+len('\\\\\\\\')] + '\\cline{2-5}' + latexTableString[latexTableString.find('\\\\\\\\')+len('\\\\\\\\'):]\n",
    "    # Find caption and put at the bottom\n",
    "    captionIndex = latexTableString.find(\"\\caption\")\n",
    "    captionEndIndex = latexTableString.find(\"}\",captionIndex+1)+1\n",
    "    endTableIndex = latexTableString.find(\"\\end{tabular}\")\n",
    "    endTableEndIndex = endTableIndex + len(\"\\end{tabular}\")\n",
    "    latexTableString[:endTableIndex] + latexTableString[endTableEndIndex:]\n",
    "    latexTableString = latexTableString[:endTableEndIndex] + latexTableString[captionIndex:captionEndIndex] + latexTableString[endTableEndIndex:]\n",
    "    latexTableString = latexTableString[:captionIndex] + latexTableString[captionEndIndex:]\n",
    "    return latexTableString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get All pcap filenames:\n",
    "def getFiles(baseDir):\n",
    "    victimFilenames = []\n",
    "    attackerFilenames = []\n",
    "    reflectorFilenames = []\n",
    "    for root, dirs, files in os.walk(baseDir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".pcapng\"):\n",
    "                if \"victim\" in file:\n",
    "                    victimFilenames.append(os.path.join(root, file))\n",
    "                if \"attacker\" in file:\n",
    "                    attackerFilenames.append(os.path.join(root, file))\n",
    "                if \"reflector\" in file:\n",
    "                    reflectorFilenames.append(os.path.join(root,file))\n",
    "    return attackerFilenames,victimFilenames,reflectorFilenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get attacker bytes sent\n",
    "def getAttackBytes(attackerFilenames,Protocol,useCachedBytes):\n",
    "    print(\"Getting Attacker Bytes for \" + Protocol + \" from pcap files...\")\n",
    "    picklePath = os.path.dirname(os.path.dirname(reflectorFilenames[0])) + '/AttackerBytes_' + Protocol + '_' + '.pkl'\n",
    "    if not useCachedBytes:\n",
    "        pbar = ProgressBar(widgets=widgets)\n",
    "        attackerBytes = []\n",
    "        for attackFile in pbar(attackerFilenames):\n",
    "            # Get attack level\n",
    "            start = attackFile.find('level')\n",
    "            end = start+len('level')+1\n",
    "            level = attackFile[start:end]\n",
    "            attackerDf = pcapToDf(attackFile,True)\n",
    "            if Protocol == \"TFTP\" or Protocol == \"SSDP\" or Protocol == \"SNMP\":\n",
    "            # By finding the TFTP packets and summing the byte lengths we get the total number of bytes send by the attacker\n",
    "                attackerDf = attackerDf.loc[attackerDf['Protocol'].isin([Protocol])]\n",
    "            attackerBytesSent = attackerDf[\"Length\"].sum()\n",
    "            attackerBytes.append({'Level':level,'Attacker Outbound':attackerBytesSent})\n",
    "        attackerBytes = pd.DataFrame(attackerBytes).sort_values('Level')\n",
    "        # Save to pickle file for fast reloading\n",
    "        attackerBytes.to_pickle(picklePath)\n",
    "    else:\n",
    "        if os.path.exists(picklePath):\n",
    "            reflectorBytes = pd.read_pickle(picklePath)\n",
    "        else:\n",
    "            raise Exception(\"Error no cached pickle file. Run the function with useCachedBytes=False to recalcuate the df, and create the pickle file '\" + os.path.basename(picklePath) + \"'\" )\n",
    " \n",
    "    return attackerBytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get Victim bytes received\n",
    "def getVictimBytes(victimFilenames,Protocol,useCachedBytes=False):\n",
    "    print(\"Getting Victim Bytes for \" + Protocol + \" from pcap files...\")\n",
    "    picklePath = os.path.dirname(os.path.dirname(reflectorFilenames[0])) + '/VictimBytes' + Protocol + '_' + '.pkl'\n",
    "    if not useCachedBytes:\n",
    "        pbar = ProgressBar(widgets=widgets)\n",
    "        victimBytes = []\n",
    "        for victimFile in pbar(victimFilenames):\n",
    "            # Get attack level\n",
    "            start = victimFile.find('level')\n",
    "            end = start+len('level')+1\n",
    "            level = victimFile[start:end]\n",
    "            victimDf = pcapToDf(victimFile,True)\n",
    "            if Protocol == \"TFTP\":\n",
    "                # In the victim pcap filtering by destination port 50040 (the tftp servers source port) gives the tftp data transfered to the victim\n",
    "                victimBytesReceived = victimDf.loc[(victimDf['UDP Destination Port'] == 50040)][\"Length\"].sum()\n",
    "            elif Protocol == \"SNMP\":\n",
    "                victimBytesReceived = victimDf.loc[victimDf['Protocol'].isin([Protocol])][\"Length\"].sum()\n",
    "            elif Protocol == \"SSDP\":\n",
    "                victimBytesReceived = victimDf.loc[(victimDf['Protocol'] == Protocol) & (victimDf['Destination'] != to_ipaddress('239.255.255.250'))][\"Length\"].sum()\n",
    "            victimBytes.append({'Level':level,'Victim Inbound':victimBytesReceived}) \n",
    "        victimBytes = pd.DataFrame(victimBytes).sort_values('Level')\n",
    "        # Save to pickle file for fast reloading\n",
    "        victimBytes.to_pickle(picklePath)\n",
    "    else:\n",
    "        if os.path.exists(picklePath):\n",
    "            reflectorBytes = pd.read_pickle(picklePath)\n",
    "        else:\n",
    "            raise Exception(\"Error no cached pickle file. Run the function with useCachedBytes=False to recalcuate the df, and create the pickle file '\" + os.path.basename(picklePath) + \"'\" )\n",
    "    return victimBytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get Reflector bytes received and sent\n",
    "def getReflectorBytes(reflectorFilenames,Protocol,useCachedBytes=False):\n",
    "    print(\"Getting Reflector Bytes for \" + Protocol + \" from pcap files...\")\n",
    "    picklePath = os.path.dirname(os.path.dirname(reflectorFilenames[0])) + '/ReflectorBytes_' + Protocol + '_' + '.pkl'\n",
    "    if not useCachedBytes:\n",
    "        pbar = ProgressBar(widgets=widgets)\n",
    "        reflectorBytes = []\n",
    "        for reflectorFile in pbar(reflectorFilenames):\n",
    "            # Get attack level\n",
    "            start = reflectorFile.find('level')\n",
    "            end = start+len('level')+1\n",
    "            level = reflectorFile[start:end]\n",
    "            reflectorDf = pcapToDf(reflectorFile,True)\n",
    "            if Protocol == \"TFTP\":\n",
    "                # In the victim pcap filtering by destination port 50040 (the tftp servers source port) gives the tftp data transfered to the victim\n",
    "                reflectorBytesReceived = reflectorDf.loc[(reflectorDf['UDP Source Port'] == 50040)][\"Length\"].sum()\n",
    "                reflectorBytesSent = reflectorDf.loc[(reflectorDf['UDP Destination Port'] == 50040)][\"Length\"].sum()\n",
    "            elif Protocol == \"SNMP\":\n",
    "                reflectorBytesReceived = reflectorDf.loc[(reflectorDf['Protocol'] == Protocol) & (reflectorDf['Info'].str.contains(\"getBulkRequest\"))][\"Length\"].sum()\n",
    "                reflectorBytesSent = reflectorDf.loc[(reflectorDf['Protocol'] == Protocol) & (reflectorDf['Info'].str.contains(\"get-response\"))][\"Length\"].sum()\n",
    "            elif Protocol == \"SSDP\":\n",
    "                reflectorBytesReceived = reflectorDf.loc[(reflectorDf['Protocol'] == Protocol) & (reflectorDf['Destination'] != to_ipaddress('239.255.255.250')) & (reflectorDf[\"Info\"].str.contains(\"M-SEARCH\"))][\"Length\"].sum()\n",
    "                reflectorBytesSent = reflectorDf.loc[(reflectorDf['Protocol'] == Protocol) & (reflectorDf['Destination'] != to_ipaddress('239.255.255.250')) & (reflectorDf[\"Info\"].str.contains(\"HTTP\"))][\"Length\"].sum()\n",
    "            reflectorBytes.append({'Level':level,'Reflector Inbound':reflectorBytesReceived,'Reflector Outbound':reflectorBytesSent})\n",
    "        reflectorBytes = pd.DataFrame(reflectorBytes).sort_values('Level')\n",
    "        # Save to pickle file for fast reloading\n",
    "        reflectorBytes.to_pickle(picklePath)\n",
    "    else:\n",
    "        if os.path.exists(picklePath):\n",
    "            reflectorBytes = pd.read_pickle(picklePath)\n",
    "        else:\n",
    "            raise Exception(\"Error no cached pickle file. Run the function with useCachedBytes=False to recalcuate the df, and create the pickle file '\" + os.path.basename(picklePath) + \"'\" )\n",
    "    return reflectorBytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data manipulation for presentation\n",
    "def getStatDf(attackerBytes,victimBytes,reflectorBytes):\n",
    "    StatDf = pd.DataFrame(columns=pd.MultiIndex.from_tuples([(\"Victim\", \"Inbound\"), (\"Reflector\", \"Inbound\"), (\"Reflector\", \"Outbound\"), (\"Attacker\", \"Outbound\")]))\n",
    "    StatDf[\"Level\"] = victimBytes['Level'].apply(lambda x: x.strip('level'))\n",
    "    StatDf[(\"Victim\",\"Inbound\")] = victimBytes['Victim Inbound']\n",
    "    StatDf[(\"Reflector\",\"Inbound\")] = reflectorBytes['Reflector Inbound']\n",
    "    StatDf[(\"Reflector\",\"Outbound\")] = reflectorBytes['Reflector Outbound']\n",
    "    StatDf[(\"Attacker\",\"Outbound\")] = attackerBytes['Attacker Outbound']\n",
    "    StatDf['Amplification factor'] = StatDf[(\"Victim\",\"Inbound\")] / StatDf[(\"Attacker\",\"Outbound\")]\n",
    "    column_to_move = StatDf.pop(\"Level\")\n",
    "    StatDf.insert(0, \"Level\", column_to_move)\n",
    "    return StatDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFTP DATA OWN TOOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pcap files\n",
    "attackerFilenames,victimFilenames,reflectorFilenames = getFiles('./pcap_tftp_own_tool')\n",
    "Protocol = \"TFTP\"\n",
    "useCachedBytes = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read pcap \n",
    "attackerBytes = getAttackBytes(attackerFilenames,Protocol=Protocol,useCachedBytes=useCachedBytes)\n",
    "victimBytes = getVictimBytes(victimFilenames,Protocol=Protocol,useCachedBytes=useCachedBytes)\n",
    "reflectorBytes = getReflectorBytes(reflectorFilenames,Protocol=Protocol,useCachedBytes=useCachedBytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying the data in latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tabulate(StatDf,headers='keys',tablefmt='fancy_grid',showindex=False))\n",
    "StatDf = getStatDf(attackerBytes,victimBytes,reflectorBytes)\n",
    "latexTableString = StatDf.to_latex(column_format='ccrrrc',index=False,caption=\"TFTP DDoS test traffic\",label=\"TFTP_Test_Traffic\",position='H') # For getting the table into the report\n",
    "latexTableString = refineLatexTableStringScientificPaper(latexTableString)\n",
    "print(latexTableString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get single index dataframe\n",
    "df1 = pd.merge(attackerBytes,victimBytes,on=\"Level\")\n",
    "df2 = pd.merge(df1,reflectorBytes,on='Level')\n",
    "# Remove 'level' string in level column\n",
    "df2[\"Level\"] = df2['Level'].apply(lambda x: x.strip('level'))\n",
    "# Get maximum value in dataframe rounded up to highest exponent (i.e. up to nearest 100M for example)\n",
    "maxN = df2.select_dtypes(include=[np.number]).max().max()\n",
    "c = 10 ** int(math.log10(maxN)) # Same number of digits as max number\n",
    "yaxisRange = [10,math.ceil(maxN/c) * c]\n",
    "\n",
    "# Plot data\n",
    "fig = px.line(df2, \n",
    "              title=Protocol + \" Test Traffic\",\n",
    "              x='Level',\n",
    "              y=['Attacker Outbound','Victim Inbound', 'Reflector Inbound','Reflector Outbound'],\n",
    "              log_y=True,\n",
    "              range_y=yaxisRange,\n",
    "              markers=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNMP DATA OWN TOOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pcap files\n",
    "attackerFilenames,victimFilenames,reflectorFilenames = getFiles('./pcap_snmp_own_tool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read pcap files\n",
    "attackerBytes = getAttackBytes(attackerFilenames,Protocol=\"SNMP\")\n",
    "victimBytes = getAttackBytes(victimFilenames)\n",
    "reflectorBytes = getAttackBytes(reflectorFilenames,Protocol=\"SNMP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying the data in latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tabulate(StatDf,headers='keys',tablefmt='fancy_grid',showindex=False))\n",
    "StatDf = getStatDf(attackerBytes,victimBytes,reflectorBytes)\n",
    "latexTableString = StatDf.to_latex(column_format='ccrrrc',index=False,caption=\"SNMP DDoS test traffic\",label=\"SNMP_Test_Traffic\",position='H') # For getting the table into the report\n",
    "latexTableString = refineLatexTableStringScientificPaper(latexTableString)\n",
    "print(latexTableString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSDP DATA OWN TOOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pcap files\n",
    "attackerFilenames,victimFilenames,reflectorFilenames = getFiles('./pcap_ssdp_own_tool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read pcap files\n",
    "attackerBytes = getAttackBytes(attackerFilenames,Protocol=\"SSDP\")\n",
    "victimBytes = getAttackBytes(victimFilenames)\n",
    "reflectorBytes = getAttackBytes(reflectorFilenames,Protocol=\"SSDP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying the data in latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tabulate(StatDf,headers='keys',tablefmt='fancy_grid',showindex=False))\n",
    "StatDf = getStatDf(attackerBytes,victimBytes,reflectorBytes)\n",
    "latexTableString = StatDf.to_latex(column_format='ccrrrc',index=False,caption=\"SSDP DDoS test traffic\",label=\"SNMP_Test_Traffic\",position='H') # For getting the table into the report\n",
    "latexTableString = refineLatexTableStringScientificPaper(latexTableString)\n",
    "print(latexTableString)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "73bce13f8028dfd8341a3b2a55c69e4fac2a631d84fa7f8fd64bf1c4038b3a67"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pcap-reading')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
